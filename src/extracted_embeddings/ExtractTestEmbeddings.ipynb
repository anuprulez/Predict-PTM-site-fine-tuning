{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa5039e-00f1-42b3-a10e-06ef42c36f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 23:40:02.859301: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-02 23:40:03.373760: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-02 23:40:04.559316: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-02 23:40:04.559584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-02 23:40:04.559603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate  # Make sure to install tabulate package\n",
    "import re\n",
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import gc\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False )\n",
    "pretrained_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "pretrained_model = pretrained_model.eval()\n",
    "\n",
    "my_train = pd.read_csv('my_train.csv')\n",
    "my_valid = pd.read_csv('my_valid.csv')\n",
    "my_test = pd.read_csv('my_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810ec9a9-c090-4e2d-95ef-6b3faa881882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protT5_features(sequence): \n",
    "    # Ensure the sequence is a string\n",
    "    sequence = str(sequence)\n",
    "\n",
    "    # Replace rare amino acids with X\n",
    "    sequence = re.sub(r\"[UZOB]\", \"X\", sequence)\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    ids = tokenizer.batch_encode_plus([sequence], add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "    \n",
    "    # Extract features from the pretrained model\n",
    "    with torch.no_grad():\n",
    "        embedding = pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Extract the last hidden state\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "    \n",
    "    # Find length\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    \n",
    "    # Select features\n",
    "    seq_emd = embedding[0][:seq_len-1]\n",
    "    \n",
    "    return seq_emd\n",
    "\n",
    "def get_input_for_embedding(fasta_file):\n",
    "    sequences = []\n",
    "    \n",
    "    for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequence = str(seq_record.seq)\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Generate ProtT5 embeddings for the sequences\n",
    "    embeddings = [get_protT5_features(seq) for seq in sequences]\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d24bdb7-3d13-4382-93fe-a98e19c9c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_emb(hf_path, test_df, extract_pos=16):\n",
    "    # Process training sequences\n",
    "    test_embedding = list()\n",
    "    for seq in test_df['sequence']:\n",
    "        pt5_all = get_protT5_features(seq)\n",
    "        embed_pos = pt5_all[extract_pos, :]\n",
    "        test_embedding.append(embed_pos)\n",
    "\n",
    "\n",
    "    # Save the training embeddings to an h5 file\n",
    "    test_X = np.array(test_embedding)\n",
    "\n",
    "    print(\"Size of X_test:\", test_X.shape)\n",
    "    \n",
    "    with h5py.File(hf_path.replace(\".h5\", \"_trial.h5\"), 'w') as hf:\n",
    "        hf.create_dataset('embedding', data=test_X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859b7560-5a04-4a69-aae7-659225e17a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_test: (50, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_Y_embedding = get_input_for_embedding('./test_Pos_Neg_Y.fasta')\n",
    "aggregate_emb(\"./embeddings/trial.h5\", my_test, extract_pos=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4af562-fa86-4f03-9278-7a814fab9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
